# Basic Implementation of Encoder of Attention Transformer (Attention Is All You Need)

ğŸ“„Paper link: Attention Is All You Need (arXiv:1706.03762)

ğŸ“‚ This Repo Includes

     âœ… Encoder implementation in PyTorch with explanations of critical concepts.
          
     ğŸ˜ï¸ A dataset on which a recommendation program was built.
          
     ğŸ¤– An apartment recommendation program using a custom Encoder built from scratch in PyTorch.


ğŸ”§ Encoder Implementation in PyTorch
    Key components covered:
    
     âœ¨ Residual Connections
      
     ğŸŒ€ Positional Encoding
      
     ğŸ§  Self-Attention and Multi-Head Attention
   
     ğŸ“ Why dot product of query and key vectors can get large and how to scale them properly

ğŸ  Dataset and Recommendation Program

   * The dataset consists of apartment listings with various columns.
   
   * For building the recommendation system, I primarily used the TopFacilities and PropertyName columns.
   
   * The model leverages the Attention Mechanism to capture relationships between facilities, creating a vector representation for each apartment by averaging the       
   embeddings of its facilities.
   
   * I used cosine similarity to find the most similar apartments and torch.topk() to retrieve the top-k most similar listings to the apartment a user is viewing.


   

        
